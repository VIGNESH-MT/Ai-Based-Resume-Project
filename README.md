AI-Powered Resume Screening with Bias Detection

Explainable â€¢ Fairness-Aware â€¢ End-to-End Hiring Intelligence Pipeline

<p align="center"> <b>Not just resume parsing.</b><br/> <b>A transparent, auditable, and fairness-aware ML system for resume screening.</b> </p> <p align="center"> <a href="#why-this-project">Why This Project</a> â€¢ <a href="#system-overview">System Overview</a> â€¢ <a href="#capabilities">Capabilities</a> â€¢ <a href="#quickstart">Quickstart</a> â€¢ <a href="#architecture">Architecture</a> â€¢ <a href="#technology">Technology</a> </p>
Why This Project

Most resume screening tools optimize accuracy alone.

That is not sufficient in real hiring systems.

In practice, resume screening models must be:

explainable to recruiters

auditable by compliance teams

measurable for bias and disparate impact

deployable in real workflows

This project demonstrates a complete, production-oriented ML pipeline that treats fairness and explainability as first-class requirements, not afterthoughts.

Hiring models should not just predict â€” they should justify.

System Overview

This repository implements an end-to-end resume screening system that:

Ingests resumes in PDF / DOCX / TXT

Extracts and preprocesses text

Featurizes resumes using TF-IDF and BERT embeddings

Trains classical ML classifiers

Explains predictions using SHAP

Evaluates fairness using Fairlearn

Exposes results through a Streamlit UI

The system is designed to be:

modular

reproducible

interpretable

extensible

Capabilities
Multi-Format Resume Ingestion

PDF

DOCX

TXT

Unified loading and preprocessing pipeline.

Feature Engineering

TF-IDF for sparse, interpretable signals

BERT embeddings (sentence-transformers/all-MiniLM-L6-v2) for semantic context

Feature strategies can be compared side-by-side.

Classification Models

Logistic Regression (baseline, interpretable)

Random Forest (non-linear benchmark)

Artifacts are versioned and persisted for reuse.

Explainability with SHAP

Local explanations for individual resumes

Global feature importance

Model-agnostic interpretation layer

Predictions are inspectable, not opaque.

Fairness & Bias Detection

Fairness metrics computed using Fairlearn

Supports sensitive attributes such as:

gender

ethnicity

custom protected attributes

Disparity analysis via MetricFrame

This enables measurable bias analysis, not assumptions.

End-to-End Streamlit App

Upload resumes

View predictions

Inspect SHAP explanations

Review fairness metrics interactively

Built for demonstration, validation, and review.

âš¡ Quickstart
1ï¸âƒ£ Create and activate a virtual environment
python -m venv .venv
. .venv/Scripts/activate
# Windows PowerShell: .\.venv\Scripts\Activate.ps1

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt
python -m nltk.downloader stopwords punkt wordnet omw-1.4

3ï¸âƒ£ (Optional) Prepare sample data

Place resumes in:

sample_data/resumes/


Supported formats:

.pdf

.docx

.txt

Create a labels file:

sample_data/labels.csv


Example:

filename,label,gender
resume1.pdf,1,F
resume2.docx,0,M
resume3.txt,1,F


Sensitive attributes are optional but required for fairness analysis.

4ï¸âƒ£ Train models
python -m src.models.train \
  --data_dir sample_data/resumes \
  --labels_csv sample_data/labels.csv \
  --output_dir artifacts

5ï¸âƒ£ Run inference on new resumes
python scripts/load_and_classify.py \
  --model_dir artifacts \
  --input files_to_score

6ï¸âƒ£ Launch Streamlit app
streamlit run app/streamlit_app.py



Architecture
src/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ loader.py            # PDF / DOCX / TXT ingestion
â”‚
â”œâ”€â”€ preprocess.py            # Text cleaning & normalization
â”‚
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ tfidf.py             # TF-IDF feature extraction
â”‚   â””â”€â”€ bert.py              # BERT embeddings
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ train.py             # Model training & persistence
â”‚   â””â”€â”€ infer.py             # Inference pipeline
â”‚
â”œâ”€â”€ explain/
â”‚   â””â”€â”€ shap_explain.py      # SHAP explainability
â”‚
â”œâ”€â”€ fairness/
â”‚   â””â”€â”€ metrics.py           # Fairlearn metrics & analysis
â”‚
scripts/
â””â”€â”€ load_and_classify.py     # CLI for batch scoring
â”‚
app/
â””â”€â”€ streamlit_app.py         # Interactive UI
â”‚
artifacts/                   # Saved models & vectorizers
sample_data/
â”œâ”€â”€ resumes/
â””â”€â”€ labels.csv


This structure cleanly separates:

data ingestion

modeling

explainability

fairness analysis

presentation layer

ğŸ›  Technology

Python

scikit-learn â€” classical ML models

sentence-transformers â€” BERT embeddings

SHAP â€” explainable AI

Fairlearn â€” fairness metrics

NLTK â€” text preprocessing

Streamlit â€” interactive UI

All libraries are selected for stability, clarity, and reproducibility.

ğŸŒŸ Why This Repository Stands Out

This is not:

a toy notebook

a black-box model

an accuracy-only demo

This is:

A complete, explainable, fairness-aware ML system for resume screening â€” designed the way real hiring systems should be built.

If you care about:

responsible AI

explainable ML

hiring fairness

deployable pipelines

ğŸ‘‰ This repository is for you.

ğŸ‘¤ Author

Vignesh Murugesan
AI / Data Science Engineer

Focus Areas
Explainable AI â€¢ Fair ML â€¢ Decision Intelligence â€¢ Responsible Hiring Systems
